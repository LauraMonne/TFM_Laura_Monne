{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lisis Estad√≠stico de M√©tricas XAI\n",
        "\n",
        "Este notebook realiza un an√°lisis estad√≠stico riguroso de las m√©tricas de explicabilidad obtenidas con Quantus.\n",
        "\n",
        "**Objetivos:**\n",
        "1. **Intervalos de Confianza del 95%**: Proporcionan una estimaci√≥n del rango probable de los valores reales de las m√©tricas.\n",
        "2. **Tests de Significaci√≥n Estad√≠stica**: Comparaci√≥n por pares de m√©todos XAI usando el test de Wilcoxon (no param√©trico, adecuado para muestras peque√±as y distribuciones no normales).\n",
        "3. **An√°lisis de Potencia Estad√≠stica**: Discusi√≥n sobre las limitaciones del tama√±o muestral y su impacto en la capacidad de detectar diferencias significativas.\n",
        "\n",
        "**Requisitos previos:**\n",
        "- Ejecutar `quantus_evaluation.py` para los 3 datasets (blood, retina, breast) con `--num_samples 100`\n",
        "- Los archivos JSON deben estar en `outputs/quantus_metrics_{dataset}.json`\n",
        "\n",
        "**Nota**: Con 100 muestras por dataset, tenemos un tama√±o muestral adecuado para an√°lisis estad√≠sticos b√°sicos, aunque la potencia para detectar diferencias peque√±as puede ser limitada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n inicial e importaciones\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import wilcoxon\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "# Definir ruta del proyecto (ajustar seg√∫n tu sistema)\n",
        "PROJECT_DIR = Path.cwd().resolve()  # Usa el directorio actual\n",
        "# Alternativa para sistemas Linux: PROJECT_DIR = Path(\"/home/TFM_Laura_Monne\").resolve()\n",
        "\n",
        "# Configuraci√≥n de rutas\n",
        "OUTPUTS_DIR = PROJECT_DIR / \"outputs\"\n",
        "OUTPUTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Lista de datasets\n",
        "datasets = [\"blood\", \"retina\", \"breast\"]\n",
        "\n",
        "# M√©tricas evaluadas\n",
        "metric_names = [\"faithfulness\", \"localization\", \"complexity\", \"randomization\", \"robustness\"]\n",
        "metric_labels = {\n",
        "    \"faithfulness\": \"Fidelidad\",\n",
        "    \"localization\": \"Localizaci√≥n\",\n",
        "    \"complexity\": \"Complejidad\",\n",
        "    \"randomization\": \"Aleatorizaci√≥n\",\n",
        "    \"robustness\": \"Robustez\"\n",
        "}\n",
        "\n",
        "# M√©todos XAI\n",
        "methods = [\"gradcam\", \"gradcampp\", \"integrated_gradients\", \"saliency\"]\n",
        "method_labels = {\n",
        "    \"gradcam\": \"Grad-CAM\",\n",
        "    \"gradcampp\": \"Grad-CAM++\",\n",
        "    \"integrated_gradients\": \"Integrated Gradients\",\n",
        "    \"saliency\": \"Saliency Maps\"\n",
        "}\n",
        "\n",
        "# Colores para visualizaci√≥n\n",
        "method_colors = {\n",
        "    \"gradcam\": \"#1b9e77\",   # teal\n",
        "    \"gradcampp\": \"#d95f02\", # orange\n",
        "    \"integrated_gradients\": \"#1f77b4\", # blue\n",
        "    \"saliency\": \"#d62728\",  # red\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Configuraci√≥n inicial completada\")\n",
        "print(f\"üìÅ Directorio de outputs: {OUTPUTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de resultados Quantus desde los archivos JSON\n",
        "\n",
        "results_by_dataset = {}\n",
        "metadata_rows = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    json_path = OUTPUTS_DIR / f\"quantus_metrics_{dataset}.json\"\n",
        "    if json_path.exists():\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            results_by_dataset[dataset] = json.load(f)\n",
        "        print(f\"‚úÖ Cargado: {dataset} ({json_path.name})\")\n",
        "\n",
        "        meta = results_by_dataset[dataset].get(\"metadata\", {})\n",
        "        metadata_rows.append({\n",
        "            \"Dataset\": dataset.upper(),\n",
        "            \"Num samples\": meta.get(\"num_samples\"),\n",
        "            \"Sample strategy\": meta.get(\"sample_strategy\"),\n",
        "            \"Seed\": meta.get(\"seed\"),\n",
        "            \"Target\": meta.get(\"target\"),\n",
        "            \"M√©todos\": \", \".join(meta.get(\"methods\", [])) if meta else None,\n",
        "        })\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  No encontrado: {json_path.name}\")\n",
        "        print(f\"   Ejecuta `python quantus_evaluation.py --dataset {dataset} --num_samples 100` antes de usar este notebook.\")\n",
        "\n",
        "if not results_by_dataset:\n",
        "    raise ValueError(\"‚ùå No se encontraron resultados Quantus. Genera primero los ficheros JSON con quantus_evaluation.py.\")\n",
        "\n",
        "if metadata_rows:\n",
        "    print(\"\")\n",
        "    print(\"Resumen de metadata:\")\n",
        "    display(pd.DataFrame(metadata_rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos individuales (scores por muestra) desde los JSON\n",
        "\n",
        "def load_individual_scores(dataset_name: str) -> dict:\n",
        "    \"\"\"\n",
        "    Carga los valores individuales (scores) por muestra para cada m√©todo y m√©trica.\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\n",
        "            method: {\n",
        "                metric: [score1, score2, ..., scoreN]  # Lista de valores por muestra\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "    results = results_by_dataset[dataset_name]\n",
        "    individual_data = {}\n",
        "    \n",
        "    for method in methods:\n",
        "        if method not in results:\n",
        "            continue\n",
        "        individual_data[method] = {}\n",
        "        for metric in metric_names:\n",
        "            method_metric = results[method].get(metric, None)\n",
        "            if method_metric is None:\n",
        "                individual_data[method][metric] = None\n",
        "            else:\n",
        "                scores = method_metric.get(\"scores\", [])\n",
        "                # Filtrar None (valores inv√°lidos) y convertir a numpy array\n",
        "                valid_scores = [s for s in scores if s is not None]\n",
        "                if len(valid_scores) == 0:\n",
        "                    individual_data[method][metric] = None\n",
        "                else:\n",
        "                    individual_data[method][metric] = np.array(valid_scores, dtype=float)\n",
        "    \n",
        "    return individual_data\n",
        "\n",
        "# Cargar datos individuales para todos los datasets\n",
        "individual_scores_by_dataset = {}\n",
        "for dataset in datasets:\n",
        "    individual_scores_by_dataset[dataset] = load_individual_scores(dataset)\n",
        "    print(f\"‚úÖ Datos individuales cargados para {dataset}\")\n",
        "\n",
        "print(f\"\\nüìä N√∫mero de muestras v√°lidas por dataset:\")\n",
        "for dataset in datasets:\n",
        "    for method in methods:\n",
        "        if method in individual_scores_by_dataset[dataset]:\n",
        "            for metric in metric_names:\n",
        "                scores = individual_scores_by_dataset[dataset][method].get(metric)\n",
        "                if scores is not None:\n",
        "                    print(f\"  {dataset}/{method}/{metric}: {len(scores)} muestras v√°lidas\")\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular Intervalos de Confianza del 95% (IC95%)\n",
        "\n",
        "def calculate_confidence_interval(scores: np.ndarray, confidence: float = 0.95) -> tuple:\n",
        "    \"\"\"\n",
        "    Calcula el intervalo de confianza usando la distribuci√≥n t de Student.\n",
        "    \n",
        "    Args:\n",
        "        scores: Array de valores\n",
        "        confidence: Nivel de confianza (default: 0.95)\n",
        "    \n",
        "    Returns:\n",
        "        (mean, lower_bound, upper_bound, sem)\n",
        "    \"\"\"\n",
        "    if scores is None or len(scores) == 0:\n",
        "        return (None, None, None, None)\n",
        "    \n",
        "    n = len(scores)\n",
        "    mean = np.mean(scores)\n",
        "    sem = stats.sem(scores)  # Error est√°ndar de la media\n",
        "    \n",
        "    # Grados de libertad\n",
        "    df = n - 1\n",
        "    \n",
        "    # Valor cr√≠tico t para IC95%\n",
        "    t_critical = stats.t.ppf((1 + confidence) / 2, df)\n",
        "    \n",
        "    # Intervalo de confianza\n",
        "    margin = t_critical * sem\n",
        "    lower = mean - margin\n",
        "    upper = mean + margin\n",
        "    \n",
        "    return (mean, lower, upper, sem)\n",
        "\n",
        "# Calcular IC95% para todos los datasets, m√©todos y m√©tricas\n",
        "confidence_intervals = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "    confidence_intervals[dataset] = {}\n",
        "    for method in methods:\n",
        "        if method not in individual_scores_by_dataset[dataset]:\n",
        "            continue\n",
        "        confidence_intervals[dataset][method] = {}\n",
        "        for metric in metric_names:\n",
        "            scores = individual_scores_by_dataset[dataset][method].get(metric)\n",
        "            if scores is not None and len(scores) > 1:\n",
        "                mean, lower, upper, sem = calculate_confidence_interval(scores)\n",
        "                confidence_intervals[dataset][method][metric] = {\n",
        "                    \"mean\": mean,\n",
        "                    \"lower_95\": lower,\n",
        "                    \"upper_95\": upper,\n",
        "                    \"sem\": sem,\n",
        "                    \"n\": len(scores)\n",
        "                }\n",
        "            else:\n",
        "                confidence_intervals[dataset][method][metric] = None\n",
        "\n",
        "print(\"‚úÖ Intervalos de confianza calculados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar tablas de Intervalos de Confianza del 95%\n",
        "\n",
        "def display_confidence_intervals_table(dataset_name: str, metric_name: str):\n",
        "    \"\"\"Muestra una tabla con IC95% para una m√©trica espec√≠fica.\"\"\"\n",
        "    data_rows = []\n",
        "    \n",
        "    for method in methods:\n",
        "        if method not in confidence_intervals[dataset_name]:\n",
        "            continue\n",
        "        ci_data = confidence_intervals[dataset_name][method].get(metric_name)\n",
        "        if ci_data is not None:\n",
        "            data_rows.append({\n",
        "                \"M√©todo\": method_labels.get(method, method),\n",
        "                \"Media\": f\"{ci_data['mean']:.4f}\",\n",
        "                \"IC95% Inferior\": f\"{ci_data['lower_95']:.4f}\",\n",
        "                \"IC95% Superior\": f\"{ci_data['upper_95']:.4f}\",\n",
        "                \"SEM\": f\"{ci_data['sem']:.4f}\",\n",
        "                \"N\": ci_data['n']\n",
        "            })\n",
        "    \n",
        "    if data_rows:\n",
        "        df_ci = pd.DataFrame(data_rows)\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Intervalos de Confianza del 95% - {dataset_name.upper()} - {metric_labels.get(metric_name, metric_name)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        display(df_ci)\n",
        "        return df_ci\n",
        "    return None\n",
        "\n",
        "# Mostrar IC95% para todas las m√©tricas y datasets\n",
        "for dataset in datasets:\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"# INTERVALOS DE CONFIANZA DEL 95% - {dataset.upper()}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    \n",
        "    for metric in metric_names:\n",
        "        display_confidence_intervals_table(dataset, metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tests de Significaci√≥n Estad√≠stica: Test de Wilcoxon (comparaci√≥n por pares)\n",
        "\n",
        "def perform_wilcoxon_tests(dataset_name: str, metric_name: str, alpha: float = 0.05) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Realiza tests de Wilcoxon (signed-rank test) para comparar m√©todos XAI por pares.\n",
        "    \n",
        "    El test de Wilcoxon es no param√©trico y adecuado para:\n",
        "    - Muestras peque√±as (n < 30)\n",
        "    - Distribuciones no normales\n",
        "    - Datos pareados\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Nombre del dataset\n",
        "        metric_name: Nombre de la m√©trica\n",
        "        alpha: Nivel de significaci√≥n (default: 0.05)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame con resultados de los tests\n",
        "    \"\"\"\n",
        "    # Obtener m√©todos disponibles\n",
        "    available_methods = [m for m in methods if m in individual_scores_by_dataset[dataset_name]]\n",
        "    available_methods = [m for m in available_methods \n",
        "                        if individual_scores_by_dataset[dataset_name][m].get(metric_name) is not None]\n",
        "    \n",
        "    if len(available_methods) < 2:\n",
        "        return None\n",
        "    \n",
        "    # Realizar comparaciones por pares\n",
        "    test_results = []\n",
        "    \n",
        "    for i, method1 in enumerate(available_methods):\n",
        "        scores1 = individual_scores_by_dataset[dataset_name][method1].get(metric_name)\n",
        "        if scores1 is None or len(scores1) == 0:\n",
        "            continue\n",
        "            \n",
        "        for method2 in available_methods[i+1:]:\n",
        "            scores2 = individual_scores_by_dataset[dataset_name][method2].get(metric_name)\n",
        "            if scores2 is None or len(scores2) == 0:\n",
        "                continue\n",
        "            \n",
        "            # Asegurar que tienen la misma longitud (tomar el m√≠nimo)\n",
        "            min_len = min(len(scores1), len(scores2))\n",
        "            s1 = scores1[:min_len]\n",
        "            s2 = scores2[:min_len]\n",
        "            \n",
        "            # Test de Wilcoxon (signed-rank test)\n",
        "            try:\n",
        "                statistic, p_value = wilcoxon(s1, s2, alternative='two-sided')\n",
        "                \n",
        "                # Determinar significaci√≥n\n",
        "                is_significant = p_value < alpha\n",
        "                \n",
        "                # Calcular diferencia de medias\n",
        "                mean_diff = np.mean(s1) - np.mean(s2)\n",
        "                \n",
        "                test_results.append({\n",
        "                    \"M√©todo 1\": method_labels.get(method1, method1),\n",
        "                    \"M√©todo 2\": method_labels.get(method2, method2),\n",
        "                    \"Media 1\": f\"{np.mean(s1):.4f}\",\n",
        "                    \"Media 2\": f\"{np.mean(s2):.4f}\",\n",
        "                    \"Diferencia\": f\"{mean_diff:.4f}\",\n",
        "                    \"Estad√≠stico W\": f\"{statistic:.2f}\",\n",
        "                    \"p-valor\": f\"{p_value:.4f}\",\n",
        "                    f\"Significativo (Œ±={alpha})\": \"S√≠\" if is_significant else \"No\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                # Si hay un error (p. ej., todas las diferencias son cero)\n",
        "                test_results.append({\n",
        "                    \"M√©todo 1\": method_labels.get(method1, method1),\n",
        "                    \"M√©todo 2\": method_labels.get(method2, method2),\n",
        "                    \"Media 1\": f\"{np.mean(s1):.4f}\",\n",
        "                    \"Media 2\": f\"{np.mean(s2):.4f}\",\n",
        "                    \"Diferencia\": f\"{np.mean(s1) - np.mean(s2):.4f}\",\n",
        "                    \"Estad√≠stico W\": \"N/A\",\n",
        "                    \"p-valor\": \"N/A\",\n",
        "                    f\"Significativo (Œ±={alpha})\": \"Error\"\n",
        "                })\n",
        "    \n",
        "    if test_results:\n",
        "        df_tests = pd.DataFrame(test_results)\n",
        "        return df_tests\n",
        "    return None\n",
        "\n",
        "# Realizar tests de Wilcoxon para todas las m√©tricas y datasets\n",
        "wilcoxon_results = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "    wilcoxon_results[dataset] = {}\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"# TESTS DE WILCOXON - {dataset.upper()}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    \n",
        "    for metric in metric_names:\n",
        "        df_tests = perform_wilcoxon_tests(dataset, metric)\n",
        "        if df_tests is not None and len(df_tests) > 0:\n",
        "            wilcoxon_results[dataset][metric] = df_tests\n",
        "            print(f\"\\n{metric_labels.get(metric, metric).upper()}:\")\n",
        "            display(df_tests)\n",
        "        else:\n",
        "            print(f\"\\n{metric_labels.get(metric, metric).upper()}: No hay datos suficientes para realizar tests\")\n",
        "            wilcoxon_results[dataset][metric] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar resultados estad√≠sticos en archivos CSV\n",
        "\n",
        "print(\"=== Guardando resultados estad√≠sticos en outputs/ ===\\n\")\n",
        "\n",
        "# Guardar intervalos de confianza\n",
        "for dataset in datasets:\n",
        "    ci_rows = []\n",
        "    for method in methods:\n",
        "        if method not in confidence_intervals[dataset]:\n",
        "            continue\n",
        "        for metric in metric_names:\n",
        "            ci_data = confidence_intervals[dataset][method].get(metric)\n",
        "            if ci_data is not None:\n",
        "                ci_rows.append({\n",
        "                    \"Dataset\": dataset,\n",
        "                    \"M√©todo\": method,\n",
        "                    \"M√©trica\": metric,\n",
        "                    \"Media\": ci_data['mean'],\n",
        "                    \"IC95_Lower\": ci_data['lower_95'],\n",
        "                    \"IC95_Upper\": ci_data['upper_95'],\n",
        "                    \"SEM\": ci_data['sem'],\n",
        "                    \"N\": ci_data['n']\n",
        "                })\n",
        "    \n",
        "    if ci_rows:\n",
        "        df_ci_all = pd.DataFrame(ci_rows)\n",
        "        ci_path = OUTPUTS_DIR / f\"quantus_confidence_intervals_{dataset}.csv\"\n",
        "        df_ci_all.to_csv(ci_path, index=False)\n",
        "        print(f\"üìÅ Guardado: {ci_path.name}\")\n",
        "\n",
        "# Guardar tests de Wilcoxon\n",
        "for dataset in datasets:\n",
        "    for metric in metric_names:\n",
        "        if metric in wilcoxon_results[dataset] and wilcoxon_results[dataset][metric] is not None:\n",
        "            df_wilcoxon = wilcoxon_results[dataset][metric]\n",
        "            wilcoxon_path = OUTPUTS_DIR / f\"quantus_wilcoxon_{dataset}_{metric}.csv\"\n",
        "            df_wilcoxon.to_csv(wilcoxon_path, index=False)\n",
        "            print(f\"üìÅ Guardado: {wilcoxon_path.name}\")\n",
        "\n",
        "print(\"\\n‚úÖ Resultados estad√≠sticos guardados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n: Intervalos de Confianza del 95% por M√©todo y M√©trica\n",
        "\n",
        "def plot_confidence_intervals(dataset_name: str, metric_name: str, figsize=(10, 6)):\n",
        "    \"\"\"Genera un gr√°fico de barras con intervalos de confianza del 95%.\"\"\"\n",
        "    data_rows = []\n",
        "    \n",
        "    for method in methods:\n",
        "        if method not in confidence_intervals[dataset_name]:\n",
        "            continue\n",
        "        ci_data = confidence_intervals[dataset_name][method].get(metric_name)\n",
        "        if ci_data is not None:\n",
        "            data_rows.append({\n",
        "                \"method\": method,\n",
        "                \"mean\": ci_data['mean'],\n",
        "                \"lower\": ci_data['lower_95'],\n",
        "                \"upper\": ci_data['upper_95'],\n",
        "                \"sem\": ci_data['sem']\n",
        "            })\n",
        "    \n",
        "    if not data_rows:\n",
        "        print(f\"No hay datos para {dataset_name} - {metric_name}\")\n",
        "        return\n",
        "    \n",
        "    df_plot = pd.DataFrame(data_rows)\n",
        "    df_plot = df_plot.sort_values('mean', ascending=True)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    \n",
        "    x_pos = np.arange(len(df_plot))\n",
        "    means = df_plot['mean'].values\n",
        "    lowers = df_plot['lower'].values\n",
        "    uppers = df_plot['upper'].values\n",
        "    \n",
        "    # Barras de error\n",
        "    errors = [means - lowers, uppers - means]\n",
        "    \n",
        "    bars = ax.barh(x_pos, means, xerr=errors, capsize=5, \n",
        "                   color=[method_colors.get(m, 'gray') for m in df_plot['method'].values],\n",
        "                   alpha=0.7, edgecolor='black', linewidth=1.2)\n",
        "    \n",
        "    ax.set_yticks(x_pos)\n",
        "    ax.set_yticklabels([method_labels.get(m, m) for m in df_plot['method'].values])\n",
        "    ax.set_xlabel(f'{metric_labels.get(metric_name, metric_name)} (IC95%)', fontsize=12)\n",
        "    ax.set_title(f'Intervalos de Confianza del 95% - {dataset_name.upper()} - {metric_labels.get(metric_name, metric_name)}', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "    ax.axvline(x=0, color='black', linewidth=0.8, linestyle='-')\n",
        "    \n",
        "    # A√±adir valores en las barras\n",
        "    for i, (mean, lower, upper) in enumerate(zip(means, lowers, uppers)):\n",
        "        ax.text(mean, i, f' {mean:.3f}', va='center', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Guardar figura\n",
        "    out_path = OUTPUTS_DIR / f\"quantus_ci_{dataset_name}_{metric_name}.png\"\n",
        "    fig.savefig(out_path, dpi=300, facecolor='white', bbox_inches='tight')\n",
        "    print(f\"‚úÖ Figura guardada: {out_path.name}\")\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Generar gr√°ficos de IC95% para m√©tricas clave\n",
        "print(\"Generando gr√°ficos de Intervalos de Confianza...\\n\")\n",
        "\n",
        "key_metrics = [\"faithfulness\", \"robustness\", \"localization\"]  # M√©tricas m√°s importantes\n",
        "\n",
        "for dataset in datasets:\n",
        "    for metric in key_metrics:\n",
        "        if metric in metric_names:\n",
        "            plot_confidence_intervals(dataset, metric)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discusi√≥n: Limitaciones de Potencia Estad√≠stica\n",
        "\n",
        "### Tama√±o Muestral y Potencia\n",
        "\n",
        "**Tama√±o muestral actual**: 100 muestras por dataset\n",
        "\n",
        "**Consideraciones**:\n",
        "\n",
        "1. **Potencia Estad√≠stica Limitada para Diferencias Peque√±as**:\n",
        "   - Con n=100, la potencia para detectar diferencias peque√±as (efectos peque√±os, d < 0.3) es limitada.\n",
        "   - Para detectar diferencias peque√±as con potencia del 80% (Œ±=0.05), se necesitar√≠an aproximadamente 200-300 muestras.\n",
        "   - Las diferencias grandes (efectos grandes, d > 0.8) son detectables con n=100.\n",
        "\n",
        "2. **Tests No Param√©tricos (Wilcoxon)**:\n",
        "   - El test de Wilcoxon es menos potente que tests param√©tricos (t-test) cuando los datos son normales.\n",
        "   - Sin embargo, es m√°s robusto ante violaciones de normalidad y adecuado para muestras peque√±as.\n",
        "   - Con n=100, el test de Wilcoxon tiene buena potencia para diferencias medianas-grandes.\n",
        "\n",
        "3. **Correcci√≥n por M√∫ltiples Comparaciones**:\n",
        "   - Se realizan m√∫ltiples tests (4 m√©todos ‚Üí 6 comparaciones por m√©trica).\n",
        "   - Sin correcci√≥n (Bonferroni, FDR), aumenta el riesgo de falsos positivos (Type I error).\n",
        "   - **Recomendaci√≥n**: Considerar correcci√≥n de Bonferroni para an√°lisis m√°s conservadores:\n",
        "     - Œ±_ajustado = Œ± / n√∫mero_de_comparaciones\n",
        "     - Para 6 comparaciones: Œ±_ajustado = 0.05 / 6 ‚âà 0.0083\n",
        "\n",
        "4. **Intervalos de Confianza del 95%**:\n",
        "   - Los IC95% proporcionan una estimaci√≥n del rango probable de los valores reales.\n",
        "   - Con n=100, los IC95% son razonablemente precisos (SEM ‚âà œÉ/‚àö100).\n",
        "   - Si los IC95% de dos m√©todos no se solapan, sugiere una diferencia significativa.\n",
        "\n",
        "### Recomendaciones para Futuros Estudios\n",
        "\n",
        "1. **Aumentar tama√±o muestral a 200-300 muestras** para mejorar la potencia estad√≠stica.\n",
        "2. **Aplicar correcci√≥n de Bonferroni o FDR** para controlar el error de tipo I en m√∫ltiples comparaciones.\n",
        "3. **Realizar an√°lisis de potencia a priori** para determinar el tama√±o muestral necesario seg√∫n el tama√±o del efecto esperado.\n",
        "4. **Considerar an√°lisis bayesiano** como alternativa complementaria para comparaciones de m√©todos.\n",
        "\n",
        "### Interpretaci√≥n de Resultados\n",
        "\n",
        "- **p-valor < 0.05**: Evidencia estad√≠stica de diferencia (sin correcci√≥n por m√∫ltiples comparaciones).\n",
        "- **IC95% no solapados**: Sugiere diferencia significativa entre m√©todos.\n",
        "- **Diferencia de medias peque√±a pero significativa**: Puede no ser cl√≠nicamente relevante; considerar tama√±o del efecto (Cohen's d).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
