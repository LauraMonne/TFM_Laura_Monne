"""
Evaluaci√≥n cuantitativa de explicabilidad usando Quantus.

Mide 5 dimensiones para varios m√©todos XAI (Grad-CAM, Grad-CAM++, IG, Saliency):
- Fidelidad      -> FaithfulnessCorrelation
- Robustez       -> AvgSensitivity
- Complejidad    -> Complexity (o Entropy)
- Aleatorizaci√≥n -> MPRT / ModelParameterRandomisation
- Localizaci√≥n   -> RegionPerturbation

Uso t√≠pico:
    python quantus_evaluation.py --dataset retina --num_samples 100 --seed 123
"""

from __future__ import annotations

import argparse
import json
import os
import random
from typing import Dict, List, Callable, Tuple

import numpy as np
import torch
from tqdm import tqdm

try:
    import quantus
except ImportError as exc:
    raise SystemExit(
        "quantus no est√° instalado. Ejecuta: pip install quantus"
    ) from exc

from prepare_data import load_datasets, get_dataset_info
from train import create_data_loaders
from xai_explanations import XAIExplainer, load_trained_model


# ============================================================
#  Reproducibilidad
# ============================================================

def set_global_seed(seed: int) -> None:
    """Establece la semilla global para reproducibilidad."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # Determinismo (puede reducir rendimiento)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ============================================================
#  Argumentos de l√≠nea de comandos
# ============================================================

# Construye el parser de argumentos.

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Evaluaci√≥n cuantitativa de XAI con Quantus (por dataset individual)."
    )
    parser.add_argument(
        "--dataset",
        required=True,
        choices=["blood", "retina", "breast"],
        help="Dataset a evaluar: blood (8 clases), retina (5 clases) o breast (2 clases).",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default=None,
        help="Ruta al checkpoint entrenado. Si no se especifica, usa results/best_model_{dataset}.pth",
    )
    parser.add_argument(
        "--data_dir",
        default="./data",
        help="Directorio con los datasets MedMNIST.",
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=30,
        help="N√∫mero de muestras del set de test a evaluar.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Batch size al recorrer el conjunto de test.",
    )
    parser.add_argument(
        "--device",
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="cpu o cuda.",
    )
    parser.add_argument(
        "--methods",
        nargs="+",
        default=["gradcam", "gradcampp", "integrated_gradients", "saliency"],
        help="M√©todos XAI a evaluar (gradcam, gradcampp, integrated_gradients, saliency).",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Ruta de guardado para los resultados. Si no se especifica, usa outputs/quantus_metrics_{dataset}.json",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=123,
        help="Seed global para reproducibilidad.",
    )
    return parser.parse_args()


# ============================================================
#  Utilidades de datos
# ============================================================

# Recopila muestras del conjunto de test.
# Devuelve un tensor BCHW (batch, channels, height, width).
# y un tensor BHWC (batch, height, width, channels).
def collect_samples(test_loader, num_samples: int, device: torch.device) -> tuple[torch.Tensor, torch.Tensor]:
    """Recopila x_batch, y_batch del conjunto de test."""
    xs: List[torch.Tensor] = []
    ys: List[torch.Tensor] = []
    with torch.no_grad():
        for idx, (data, target) in enumerate(tqdm(test_loader, desc="Recolectando muestras")):
            if idx >= num_samples:
                break
            xs.append(data)
            ys.append(target)
    if not xs:
        raise RuntimeError("No se encontraron muestras. Ajusta num_samples.")
    x_batch = torch.cat(xs, dim=0).to(device)
    y_batch = torch.cat(ys, dim=0).to(device)
    return x_batch, y_batch

# Convierte un tensor BCHW a BHWC para Quantus.
def to_numpy_bchw(tensor_batch: torch.Tensor) -> np.ndarray:
    """Convierte un tensor BCHW a NumPy BCHW (sin cambiar el orden de ejes)."""
    return tensor_batch.detach().cpu().numpy()


# ============================================================
#  Sanitizaci√≥n / normalizaci√≥n de atribuciones
# ============================================================

def sanitize_attribution(attr: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """
    Sanitiza y normaliza atribuciones para mejorar estabilidad num√©rica.
    
    - Fuerza float32 para consistencia
    - Reemplaza nan/inf por 0
    - Normaliza por muestra a [0,1] (min-max) para evitar mapas constantes raros
    
    Args:
        attr: Tensor de atribuciones (C, H, W) o (B, C, H, W)
        eps: Tolerancia para detectar mapas constantes
        
    Returns:
        Tensor sanitizado y normalizado
    """
    attr = attr.to(dtype=torch.float32)
    attr = torch.nan_to_num(attr, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Si todo es cero (mapa vac√≠o), devolvemos tal cual
    if torch.all(attr == 0):
        return attr
    
    # Normalizaci√≥n min-max por tensor (C,H,W) o (B,C,H,W)
    if attr.ndim == 3:  # (C, H, W)
        mn = torch.min(attr)
        mx = torch.max(attr)
        if (mx - mn).abs() < eps:
            # Mapa constante -> lo dejamos a ceros
            return torch.zeros_like(attr)
        attr = (attr - mn) / (mx - mn + eps)
    elif attr.ndim == 4:  # (B, C, H, W) - normalizar por muestra
        for b in range(attr.shape[0]):
            sample = attr[b]
            mn = torch.min(sample)
            mx = torch.max(sample)
            if (mx - mn).abs() < eps:
                attr[b] = torch.zeros_like(sample)
            else:
                attr[b] = (sample - mn) / (mx - mn + eps)
    else:
        # Formato no soportado, devolver tal cual
        return attr
    
    return attr


# ============================================================
#  Atribuciones XAI (reutiliza XAIExplainer)
# ============================================================

# Expande un heatmap HxW a CxHxW repitiendo por canal.
def expand_heatmap_to_channels(heatmap: np.ndarray, channels: int) -> torch.Tensor:
    """Expande un heatmap HxW a CxHxW repitiendo por canal."""
    if heatmap.ndim != 2:
        raise ValueError("El heatmap debe ser 2D.")
    tensor = torch.tensor(heatmap, dtype=torch.float32)
    tensor = tensor.unsqueeze(0).repeat(channels, 1, 1)  # (C, H, W)
    return tensor


def compute_attributions(
    explainer: XAIExplainer,
    x_batch: torch.Tensor,
    preds: torch.Tensor,
    method: str,
) -> torch.Tensor:
    """
    Genera atribuciones para todo el batch usando el m√©todo especificado.
    Devuelve un tensor BCHW (batch, channels, height, width).
    """
    attributions: List[torch.Tensor] = []
    for idx in tqdm(range(len(x_batch)), desc=f"Atribuciones {method}"):
        sample = x_batch[idx : idx + 1]
        target_class = int(preds[idx].item())
        try:
            if method == "gradcam":
                result = explainer.generate_gradcam(sample, target_class, save_path=None)
                if result is None:
                    raise RuntimeError("Grad-CAM retorn√≥ None")
                _, heatmap = result
                attr = expand_heatmap_to_channels(heatmap, sample.shape[1])
            elif method == "gradcampp":
                result = explainer.generate_gradcampp(sample, target_class, save_path=None)
                if result is None:
                    raise RuntimeError("Grad-CAM++ retorn√≥ None")
                _, heatmap = result
                attr = expand_heatmap_to_channels(heatmap, sample.shape[1])
            elif method == "integrated_gradients":
                result = explainer.generate_integrated_gradients(sample, target_class, save_path=None)
                if result is None:
                    raise RuntimeError("IG retorn√≥ None")
                attr = result[1][0].detach().cpu()  # (C, H, W)
            elif method == "saliency":
                result = explainer.generate_saliency_map(sample, target_class, save_path=None)
                if result is None:
                    raise RuntimeError("Saliency retorn√≥ None")
                attr = result[1][0].detach().cpu()  # (C, H, W)
            else:
                raise ValueError(f"M√©todo desconocido: {method}")
            
            # Sanitizar atribuci√≥n antes de a√±adirla
            attr = sanitize_attribution(attr)
        except Exception as err:
            print(f"‚ö†Ô∏è Error generando atribuci√≥n para muestra {idx}: {err}")
            attr = torch.zeros_like(sample[0].cpu())
        attributions.append(attr)
    return torch.stack(attributions, dim=0)  # (B, C, H, W)


# ============================================================
#  explain_func para m√©tricas que lo requieren (robustness, randomization)
# ============================================================


def build_explain_func(
    explainer: XAIExplainer,
    method: str,
    device: torch.device,
) -> Callable:
    """
    Construye una explain_func compatible con Quantus.
    Firma esperada: explain_func(model, inputs, targets, **kwargs) -> np.ndarray
    """

    def explain_func(model, inputs, targets, **kwargs):
        # inputs puede venir como np.ndarray o torch.Tensor, BCHW o BHWC
        if isinstance(inputs, np.ndarray):
            x = torch.tensor(inputs, dtype=torch.float32)
        else:
            x = inputs

        if x.ndim == 4 and x.shape[-1] in (1, 3):  # BHWC -> BCHW
            x = x.permute(0, 3, 1, 2)

        x = x.to(device)

        if isinstance(targets, np.ndarray):
            y = torch.tensor(targets, dtype=torch.long, device=device)
        else:
            y = targets.to(device)

        attributions: List[torch.Tensor] = []
        for i in range(len(x)):
            sample = x[i : i + 1]
            target_class = int(y[i].item())
            try:
                if method == "gradcam":
                    result = explainer.generate_gradcam(sample, target_class, save_path=None)
                    if result is None:
                        raise RuntimeError("Grad-CAM retorn√≥ None")
                    _, heatmap = result
                    attr = expand_heatmap_to_channels(heatmap, sample.shape[1])
                elif method == "gradcampp":
                    result = explainer.generate_gradcampp(sample, target_class, save_path=None)
                    if result is None:
                        raise RuntimeError("Grad-CAM++ retorn√≥ None")
                    _, heatmap = result
                    attr = expand_heatmap_to_channels(heatmap, sample.shape[1])
                elif method == "integrated_gradients":
                    result = explainer.generate_integrated_gradients(sample, target_class, save_path=None)
                    if result is None:
                        raise RuntimeError("IG retorn√≥ None")
                    attr = result[1][0].detach().cpu()
                elif method == "saliency":
                    result = explainer.generate_saliency_map(sample, target_class, save_path=None)
                    if result is None:
                        raise RuntimeError("Saliency retorn√≥ None")
                    attr = result[1][0].detach().cpu()
                else:
                    raise ValueError(f"M√©todo desconocido: {method}")
                
                # Sanitizar atribuci√≥n antes de a√±adirla
                attr = sanitize_attribution(attr)
            except Exception as err:
                print(f"‚ö†Ô∏è Error en explain_func para muestra {i}: {err}")
                attr = torch.zeros_like(sample[0].cpu())
            attributions.append(attr)

        # Devuelve BCHW como NumPy
        return torch.stack(attributions, dim=0).detach().cpu().numpy()

    return explain_func


# ============================================================
#  M√©tricas de Quantus
# ============================================================

# Crea un conjunto est√°ndar de m√©tricas de Quantus.
# Devuelve un diccionario con las m√©tricas.
def create_metrics() -> Dict[str, object]:
    """Crea un conjunto est√°ndar de m√©tricas de Quantus."""
    metrics: Dict[str, object] = {}

    # Fidelidad
    metrics["faithfulness"] = quantus.FaithfulnessCorrelation()

    # Robustez
    # Configuraci√≥n para evitar valores inf/nan:
    # - return_nan_when_prediction_changes=True: devuelve nan en lugar de inf cuando la predicci√≥n cambia
    # - nr_samples=30: reduce el n√∫mero de muestras para evitar problemas num√©ricos (default=200)
    # - lower_bound=0.02: ruido m√≠nimo muy peque√±o para evitar cambios de predicci√≥n
    # - upper_bound=0.15: ruido m√°ximo peque√±o para mantener predicciones estables
    # - abs=True: usa valores absolutos para evitar problemas con signos
    # - normalise=True: normaliza las explicaciones para estabilidad num√©rica
    # - similarity_func: usar correlaci√≥n en lugar de distancia euclidiana (m√°s robusta)
    metrics["robustness"] = quantus.AvgSensitivity(
        nr_samples=30,  # Reducir muestras para evitar problemas num√©ricos
        abs=True,  # Usar valores absolutos
        normalise=True,  # Normalizar para estabilidad
        lower_bound=0.02,  # Ruido m√≠nimo muy peque√±o para evitar cambios de predicci√≥n
        upper_bound=0.15,  # Ruido m√°ximo peque√±o para mantener predicciones estables
        return_nan_when_prediction_changes=True,  # Devolver nan en lugar de inf
        disable_warnings=True,  # Desactivar warnings para limpieza
    )

    # Complejidad o Entropy
    try:
        metrics["complexity"] = quantus.Complexity()
    except AttributeError:
        metrics["complexity"] = quantus.Entropy()

    # Aleatorizaci√≥n (MPRT o ModelParameterRandomisation)
    # MPRT mide c√≥mo cambian las explicaciones cuando se aleatorizan los par√°metros del modelo.
    # Valores cercanos a 1.0 indican que las explicaciones no cambian (malo - explicaciones no son sensibles).
    # Valores cercanos a 0.0 indican que las explicaciones cambian significativamente (bueno - explicaciones son sensibles).
    # 
    # PROBLEMA: Si todos los m√©todos obtienen ~1.0, puede ser que:
    # 1. La m√©trica no est√© aleatorizando correctamente los par√°metros
    # 2. Las explicaciones realmente no cambian cuando se aleatorizan par√°metros (problema de los m√©todos XAI)
    # 3. Hay un bug en c√≥mo se est√° usando la m√©trica
    #
    # SOLUCI√ìN: Probar diferentes configuraciones y m√©tricas alternativas
    try:
        RandomizationMetric = quantus.MPRT
        print("üìä Usando m√©trica MPRT para randomization")
    except AttributeError:
        RandomizationMetric = quantus.ModelParameterRandomisation
        print("üìä Usando m√©trica ModelParameterRandomisation para randomization")
    
    # Intentar configurar con par√°metros que ayuden a diferenciar m√©todos
    randomization_metric = None
    config_attempts = [
        # Configuraci√≥n 1: Con funci√≥n de similitud expl√≠cita (correlaci√≥n de Spearman)
        {
            "name": "correlaci√≥n Spearman + normalizaci√≥n",
            "params": {
                "similarity_func": quantus.similarity_func.correlation_spearman,
                "normalise": True,
                "disable_warnings": True,
            }
        },
        # Configuraci√≥n 2: Con correlaci√≥n de Pearson
        {
            "name": "correlaci√≥n Pearson + normalizaci√≥n",
            "params": {
                "similarity_func": quantus.similarity_func.correlation_pearson,
                "normalise": True,
                "disable_warnings": True,
            }
        },
        # Configuraci√≥n 3: Solo normalizaci√≥n
        {
            "name": "solo normalizaci√≥n",
            "params": {
                "normalise": True,
                "disable_warnings": True,
            }
        },
        # Configuraci√≥n 4: Por defecto
        {
            "name": "por defecto",
            "params": {}
        }
    ]
    
    for attempt in config_attempts:
        try:
            randomization_metric = RandomizationMetric(**attempt["params"])
            print(f"   ‚úì Configuraci√≥n exitosa: {attempt['name']}")
            break
        except (TypeError, AttributeError, KeyError) as e:
            continue
    
    if randomization_metric is None:
        # Si todas las configuraciones fallan, usar la m√°s b√°sica
        print("   ‚ö†Ô∏è  Todas las configuraciones fallaron, usando configuraci√≥n m√≠nima")
        randomization_metric = RandomizationMetric()
    
    metrics["randomization"] = randomization_metric

    # Localizaci√≥n
    metrics["localization"] = quantus.RegionPerturbation()

    # NOTA: Si MPRT sigue dando valores constantes (~1.0) para todos los m√©todos,
    # se puede considerar usar m√©tricas alternativas como:
    # - quantus.RandomLogit: Mide la distancia entre explicaci√≥n original y una clase aleatoria
    # - Una m√©trica personalizada que compare explicaciones con diferentes seeds
    # Sin embargo, MPRT es la m√©trica est√°ndar para randomization, as√≠ que primero
    # intentamos con las configuraciones mejoradas arriba.

    return metrics

# Eval√∫a cada m√©todo XAI con varias m√©tricas de Quantus.
# Devuelve un diccionario: results[method][metric] = {mean, std, scores}.
def evaluate_methods(
    model: torch.nn.Module,
    explainer: XAIExplainer,
    x_batch: torch.Tensor,
    y_batch: torch.Tensor,
    methods: list[str],
    device: torch.device,
) -> Dict[str, Dict[str, Dict[str, float]]]:
    """
    Eval√∫a cada m√©todo XAI con varias m√©tricas de Quantus.
    Devuelve un diccionario: results[method][metric] = {mean, std, scores}.
    """
    model.eval()
    metrics = create_metrics()

    # Predicciones del modelo (para usar como clases objetivo)
    with torch.no_grad():
        logits = model(x_batch.to(device))
        preds = logits.argmax(dim=1)

    # Convertir datos a NumPy (manteniendo BCHW)
    x_np = to_numpy_bchw(x_batch)  # (B, C, H, W)
    y_np = y_batch.detach().cpu().numpy()

    results: Dict[str, Dict[str, Dict[str, float]]] = {}

    for method in methods:
        print(f"\n=== Evaluando m√©todo XAI: {method} ===")

        # 1) Atribuciones (mantenemos BCHW para coincidir con x_np)
        attr_bchw = compute_attributions(explainer, x_batch, preds, method)
        attr_np = to_numpy_bchw(attr_bchw)  # (B, C, H, W)

        method_results: Dict[str, Dict[str, float]] = {}

        # explain_func para m√©tricas que lo requieren (robustness, randomization)
        explain_fn = build_explain_func(explainer, method, device)

        for metric_name, metric in metrics.items():
            print(f" -> M√©trica: {metric_name}")
            try:
                # Robustez y aleatorizaci√≥n: usar explain_func (Quantus calcula a_batch internamente)
                if metric_name in {"robustness", "randomization"}:
                    # Logging adicional para randomization
                    if metric_name == "randomization":
                        print(f"    üîç Calculando randomization (MPRT) para {method}...")
                        print(f"       Esto puede tardar ya que aleatoriza par√°metros del modelo.")
                    
                    scores = metric(
                        model=model,
                        x_batch=x_np,
                        y_batch=y_np,
                        explain_func=explain_fn,
                        device=device,
                    )
                else:
                    # Resto de m√©tricas: usar atribuciones precomputadas (a_batch)
                    scores = metric(
                        model=model,
                        x_batch=x_np,
                        y_batch=y_np,
                        a_batch=attr_np,
                        device=device,
                    )

                # Algunas m√©tricas (p.ej. randomization) pueden devolver un dict.
                if isinstance(scores, dict):
                    # Caso t√≠pico: clave 'scores'
                    if "scores" in scores:
                        raw_scores = scores["scores"]
                    else:
                        # Buscar el primer valor que parezca una colecci√≥n num√©rica
                        raw_scores = None
                        for v in scores.values():
                            if isinstance(v, (list, tuple, np.ndarray)):
                                raw_scores = v
                                break
                        if raw_scores is None:
                            raise TypeError(
                                f"Formato de salida de m√©trica '{metric_name}' no soportado: claves={list(scores.keys())}"
                            )
                else:
                    raw_scores = scores

                raw_scores = np.array(raw_scores, dtype=float).flatten()
                
                # Detecci√≥n especial para randomization: verificar si todos los valores son constantes
                if metric_name == "randomization" and len(raw_scores) > 0:
                    valid_for_check = raw_scores[np.isfinite(raw_scores)]
                    if len(valid_for_check) > 0:
                        # Verificar si todos los valores son muy cercanos a 1.0 (constantes)
                        all_near_one = np.all(np.abs(valid_for_check - 1.0) < 0.01)
                        if all_near_one:
                            print(f"    ‚ö†Ô∏è  ADVERTENCIA: Todos los valores de randomization est√°n cerca de 1.0")
                            print(f"       Esto indica que las explicaciones no cambian cuando se aleatorizan los par√°metros.")
                            print(f"       Posibles causas:")
                            print(f"       1. Los m√©todos XAI no son sensibles a la aleatorizaci√≥n de par√°metros")
                            print(f"       2. La m√©trica MPRT no est√° funcionando correctamente")
                            print(f"       3. El modelo es demasiado robusto a la aleatorizaci√≥n")
                
                # Filtrar inf y nan antes de calcular estad√≠sticas
                valid_scores = raw_scores[np.isfinite(raw_scores)]  # isfinite = no inf y no nan
                
                if len(valid_scores) == 0:
                    # Si todos los valores son inf/nan, usar None
                    mean = None
                    std = None
                    print(f"    ‚ö†Ô∏è  Todos los valores son inf/nan, usando None")
                elif len(valid_scores) < len(raw_scores):
                    # Si hay algunos valores v√°lidos, calcular solo con ellos
                    mean = float(np.mean(valid_scores))
                    std = float(np.std(valid_scores))
                    invalid_count = len(raw_scores) - len(valid_scores)
                    print(f"    ‚ö†Ô∏è  {invalid_count}/{len(raw_scores)} valores inv√°lidos (inf/nan) filtrados")
                else:
                    # Todos los valores son v√°lidos
                    mean = float(np.mean(valid_scores))
                    std = float(np.std(valid_scores))
                
                # Convertir inf y nan a None para JSON
                mean_json = None if (mean is None or (mean is not None and (np.isinf(mean) or np.isnan(mean)))) else mean
                std_json = None if (std is None or (std is not None and (np.isinf(std) or np.isnan(std)))) else std
                
                # Convertir scores: inf -> None, nan -> None
                scores_list = []
                for s in raw_scores:
                    if np.isinf(s) or np.isnan(s):
                        scores_list.append(None)
                    else:
                        scores_list.append(float(s))

                method_results[metric_name] = {
                    "mean": mean_json,
                    "std": std_json,
                    "scores": scores_list,
                }
                
                # Print con manejo de inf/nan - mostrar valores reales o advertencia
                if mean_json is None:
                    if len(valid_scores) == 0:
                        print(f"    None (todos los valores son inf/nan)")
                    else:
                        print(f"    None (filtrados {len(raw_scores) - len(valid_scores)}/{len(raw_scores)} valores inv√°lidos)")
                elif std_json is None:
                    print(f"    {mean:.4f} ¬± None")
                else:
                    print(f"    {mean:.4f} ¬± {std:.4f}")
            except Exception as err:
                print(f"    ‚ö†Ô∏è Error evaluando {metric_name} para {method}: {err}")
                method_results[metric_name] = None

        results[method] = method_results

    return results

# Guarda los resultados en un archivo JSON.
def save_results(results: Dict, output_path: str) -> None:
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as file:
        json.dump(results, file, indent=2, ensure_ascii=False)
    print(f"\n‚úÖ Resultados guardados en {output_path}")


# ============================================================
#  main()
# ============================================================

# Funci√≥n principal
def main() -> None:
    args = parse_args()
    device = torch.device(args.device)
    
    # Establecer semilla para reproducibilidad
    set_global_seed(args.seed)

    print("=" * 60)
    print("  EVALUACI√ìN QUANTUS - RESNET18 XAI")
    print("=" * 60)
    print(f"Dataset: {args.dataset}")
    print(f"Dispositivo: {device}")
    print(f"M√©todos: {args.methods}")
    print(f"Muestras a evaluar: {args.num_samples}")
    print(f"Seed: {args.seed}")

    # Determinar n√∫mero de clases seg√∫n dataset
    meta_all = get_dataset_info()
    name_map = {"blood": "bloodmnist", "retina": "retinamnist", "breast": "breastmnist"}
    med_name = name_map[args.dataset]
    num_classes = int(meta_all[med_name]["n_classes"])

    # Determinar ruta del modelo
    if args.model_path is None:
        model_path = f"results/best_model_{args.dataset}.pth"
    else:
        model_path = args.model_path

    if not os.path.exists(model_path):
        raise FileNotFoundError(
            f"No se ha encontrado {model_path}. "
            f"Ejecuta primero: python train.py --dataset {args.dataset}"
        )

    # Modelo entrenado
    model = load_trained_model(model_path, device, num_classes=num_classes)

    # Datos de test
    datasets = load_datasets(args.data_dir, target_size=224)
    _, _, test_loader, _ = create_data_loaders(
        datasets=datasets,
        batch_size=args.batch_size,
        num_workers=0,
        num_classes=num_classes,
        dataset_name=args.dataset,
    )

    # Muestreo de ejemplos de test
    x_batch, y_batch = collect_samples(test_loader, args.num_samples, device)

    # Explainer XAI
    explainer = XAIExplainer(model, device, num_classes=num_classes)

    # Evaluaci√≥n
    results = evaluate_methods(model, explainer, x_batch, y_batch, args.methods, device)
    
    # A√±adir metadata al resultado
    results["metadata"] = {
        "dataset": args.dataset,
        "num_classes": num_classes,
        "num_samples": args.num_samples,
        "methods": args.methods,
    }
    
    # Determinar ruta de salida
    if args.output is None:
        output_path = f"outputs/quantus_metrics_{args.dataset}.json"
    else:
        output_path = args.output
    
    save_results(results, output_path)
    print(f"\n‚úÖ Resultados guardados en: {output_path}")


if __name__ == "__main__":
    main()

"""
Resumen
El script quantus_evaluation.py eval√∫a la explicabilidad de un modelo entrenado con varios m√©todos XAI (Grad-CAM, Grad-CAM++, Integrated Gradients y Saliency) usando las m√©tricas de Quantus.
1. Argumentos: lee los par√°metros de l√≠nea de comandos.
2. Datos: carga los datasets MedMNIST y crea un loader de test.
3. Muestreo: recoge un batch de muestras del conjunto de test.
4. Explainer: inicializa el objeto XAIExplainer.
5. Evaluaci√≥n: llama a evaluate_methods() para cada m√©todo XAI.
6. Guarda: guarda los resultados en un archivo JSON.

Resultado: un archivo JSON con los resultados de la evaluaci√≥n cuantitativa de la explicabilidad.
"""